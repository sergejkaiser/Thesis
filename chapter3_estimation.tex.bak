\section{Estimation method} 
In this section I explain the issues concerning the estimation of the dispersion parameter $\theta$ in the equation \ref{eq:1}, which is the first step to obtain the structural RCA measure. First, I am going to discuss potential violations of the OLS assumptions in the estimation. Second, I am discussing the effect of missing data in the estimation and the imputation method I chose to impute.\par %Moreover, I discuss a robustness check of the log linear specification of the regression with a Poisson pseudo maximum likelihood estimation as proposed in \textcite{silva}. \par
%\subsection{Endogeneity and Errors in Variables}
%\textcite{costinot} estimate the regression in equation \ref{eq:1} with OLS and IV methods.\par
The OLS estimate of $\theta$ is unbiased  and consistent if the regressor inverse producer price is uncorrelated with the error term. The error term includes variable trade cost and other unobserved time varying factors. The error term may be correlated with the regressor because of two reasons conditions, the simultaneity bias and measurement error in the producer prices \parencite{Costinot}. \par 
First, the simultaneity bias may be due to agglomeration effects, e.g. if the presence of an exporting firm allocated in a spacial proximity has a positive spillover on another firms exports \parencite{bernard2004}. The sign of the bias is apriori ambiguous \parencite{costinot}.     \par
Second, a classical measurement error may also cause the producer price to be endogenous. If the measurement error is correlated with the true underlying variable, the estimate would be biased towards zero \parencite[p.85[{greene}. Therefore I expect that the estimate of $\theta$ price variable is that the OLS estimate $\theta$  would be lower than the true value. A solution to the endogeneity problem is the  instrumental variable regression \parencite[p.139]{Dhaene}. \par
Another motivation for the IV estimation is that under a more general cost function the producer prices may reflect other comparative advantage sources than productivity. Using the variation of the observed producer prices only related R\&D expenditure may help to improve the identification of the effects of productivity on gross exports. The choice of the instrument is the same as in \textcite{Costinot} and \textcite{Eaton}. \par
A valid instrument for the IV estimation has to satisfy two assumptions  the exclusion and the relevance assumption \parencite{cameron2009}. The exclusion assumption requires that the instrument is uncorrelated with the error term. The relevance assumption requires that  the instrument shows a strong correlation with the endogenous regressor. In the next paragraph I discuss the consequences if the second assumption is violated.\par 
The violation of assumption two is termed a weak instruments problem. A consequence of a weak instrument is that the IV estimator does not  identify the causal effect of the endogenous variable  \parencite{bound}. Further, if the instrument is weak, even a small correlation of the instrument and the error term may lead to a larger  bias in the IV estimation than OLS \parencite{bound}. A rule-of thumb exceeds to rule out out a weak instrument problem is that the excluded instrument has an F-statistic exceeding 10 \textcite{mastering}.    \par

%The Kleibergen-Paap rk Wald F-statistic of \textcite{Kleibergen06} can be used to test for a weak identification problem. The test statistic is  a heteroscedasticity-robust. The critical values for a test using this test-statistic at the 5 \% level that relative bias of the IV is lower than ten percentage is 16.38. Although the thresholds was obtained by \textcite{stock} for a homoscedastic alternative of the test statistic, it may be used with caution for the robust alternative  \parencite{Baum07}. \par %critical values for the Cragg-Donald-Wald F-test statistic, such that the relative bias of the IV estimate compared to OLS is below a certain threshold. The critical value for the test of a significance of 5\% that the IV bias relative to the OLS bias is less than ten percent is 16.38 \parencite{stock}. . \par
%\subsection{Poisson maximum likelihood}
\subsection{Missing data imputation}
%First motivate why multiple imputation
%second sketch technicql 
 %motivation still sketchy 
 In our data set we observe missing data in the R \& D and the producer prices. Important concerns about missing data are (1) efficiency losses (2) complications in data handling and data analysis (3) bias due to differences between the observed and unobserved data \textcite{Schaffer}. Further, in the IV estimation missing data in the instrument may reduce the strength of the association between the instrument and the instrumented variable and therefore may lead to upward biased estimates. For these reasons I will impute the instrumented and instrumental variable. In the following I will  motivate the choice and then give an description of the technique.  \par 
 %motivate choice of technqiue
The choice of multiple imputation is motivated as follows. First,  ignoring missingnes implicitly make even stronger assumptions that the mechanism of missingness is orthogonal to the analysis. Further, simple solutions as e.g. single imputation have the draw back that they do not take into account the uncertainty induced by the missing values and hence any analysis conducted after a single imputation will show to high certainty \parencite{Schaffer}.  Last, alternative techniques as maximum likelihood require case specific adjustment and are difficult to implement \parencite{Schaffer}. The benefits of multiple imputation is that it is a general approach to deal with missing data, which correctly accounts for the uncertainty induced by missing observations. \par
 In general multiple imputation is a two step procedure \parencite{brownstone2001mi}. In a first step the missing values are imputed using simulations and imputed with $m$ plausible values according to the imputation model.  In the next step the analysis is done with complete data methods and the results are combined with rules described in \textcite(p.77){Rubin1987}.  \par The validity of multiple imputation requires making assumptions about the causes and effects of missing data. Multiple imputation is implemented in such a way that it is assumed that missingness is ignorable. Ignorable here means that the probability of an missing observations does not depend on unobserved variables if we control for observed variables. In substance, assuming ignoaribility of the missing observations implies that I belief that the effect of the missing observations can be corrected with the given complete observations \textcite(p.223){van2007multiple} %Van Buren The assumption of ignorability is essentially the belief on the part of the user that the available data are sufficient to correct for the effects of the missing data. The assump- tion cannot be tested on the data itself, but it can be checked against suitable external validation data 
  \par  Especially I will use multiple imputation with a fully conditional specification for each variable to replace the missing values. In this approach I specify for each variable with missing data a conditional specification. Another possible approach would be to specify a joint distribution of the missing variables and the imputation model.  I choose the fully conditional specification as it is more flexible approach and it allows to specify a more credible imputation model at the level of a variable instead of joint distribution \textcite{van2000mice} . A draw back of the fully conditional approach is that \par Moreover the imputation algorithm I chose is the predictive mean matching. This approach replace the missing values with draws from the observed data. As a consequence, the distribution of the imputed variables closely resembles the observed variables.  The imputation method is especially well suited to deal with skewed variables, for which normality assumptions are not well suited. \par The imputation method first imputes each missing observation with a random draw from the observed values. Second for the first variable on the variables we obtain the regression coefficients and the estimated variance. Next $\beta*$ is obtained by random draws from multivariate normal distribution with the mean equal to the vector of regression coefficients and the variance equal to the estimated variance, we obtain a new measure. The algorithm identifies the $q$ \footnote{I specified the algorithm such that it identifies the 10 closest observations. This choice is based on the results in a simulation study \textcite{Morris2014}.}, observations with the smallest difference between the mean of the regression and the multivariate normal distribution. Of these $q$ observations one observations is randomly chosen. The process of imputing the variables is called a cycle, which is repeated for several times for each of the $m$ imputed data sets. The imputed first variable is used in the imputation of the second variable. As a consequence the fully conditional specification converges to the joint distribution of the imputed and the for imputation used variables (ref. necessary).  
  
%  In the first step each missing value is imputed with value from random sampling with replacement from the observed values.  
%"Multiple imputation using chained equations: Issues and guidance for practice" Ian R.White,a ?? Patrick Roystonb and AngelaM.Woodc
 %Initially, all missing values are filled in by simple random sampling with replacement from
%the observed values. The first variable with missing values, x1 say, is regressed on all other variables x2, . . . , xk , restricted
%to individuals with the observed x1. Missing values in x1 are replaced by simulated draws from the corresponding
%posterior predictive distribution of x1. Then, the next variable with missing values, x2 say, is regressed on all other
%variables x1, x3, . . . , xk , restricted to individuals with the observed x2, and using the imputed values of x1. Again, missing
%values in x2 are replaced by draws from the posterior predictive distribution of x2. The process is repeated for all other
%variables with missing values in turn: this is called a cycle. In order to stabilize the results, the procedure is usually
%repeated for several cycles (e.g. 10 or 20) to produce a single imputed data set, and the whole procedure is repeated m
%times to give m imputed data sets.
% 4.2. Predictive mean matching
%Predictive mean matching (PMM) is an ad hoc method of imputing missing values of z with the property that imputed
%values are sampled only from the observed values of z [22]. The consequence is that the distribution of imputed z often
%closely matches that of the observed z. Such a property is desirable, for example, when z is continuous and the Normality
%assumption is untenable, or when the relationship between z and x is non-linear. It is undesirable when imputation
%appropriately involves extrapolation beyond the range of the observed values of z, or possibly when the sample size is
%small (since then only a small range of imputed values is available).
%To use PMM, the standard method described in Section 2.1 is used to give a perturbed parameter vector b
%?. For each missing value zi with covariates xi , the standard procedure would next sample from a Normal distribution with
%mean b?xi . Instead, PMM identifies the q individuals with the smallest values of |b*x_h?b*x^*_i | (h=1, . . . ,nobs). Any
%ties are broken at random. One of these q closest individuals, say i , is chosen at random, and the imputed value of
%zi is zi  . Thus the imputed value is an observed value of z whose prediction with the observed data closely matches
%the perturbed prediction. We use q=3, which performed well in a simulation study, although a more complex adaptive
%method performed better [23].
%  The MI technique consists of three steps:
%1 Imputation. Replace missing values with M sets of plausible values according to an imputation model (e.g., Rubin 1987; Schafer 1997) to create M completed datasets.
%2 Completed-data analysis. Perform primary analysis on each imputed (completed) dataset to obtain a set of completed-data estimates q?i and their respective VCEs U?i, i = 1,...,M.
%3 Pooling. Consolidate results from the completed-data analyses {q?i,U?i}Mi=1 into one MI inference using Rubin?s combination rules (e.g. Rubin 1987, 76).
%mi impute assumes that missing data are missing at random; that is, missing values do not carry any extra information about why they are missing than what is already available in the observed data.
%mi impute creates imputations by simulating from a (approximate) Bayesian posterior predictive distribution of the missing data, following Rubin?s recommendation.
%Methods and formulas
%mi impute pmm follows the steps as described in Methods and formulas of [MI] mi impute regress
%with the exception of step 3.
%Consider a univariate variable x = (x1, x2, . . . , xn)
%0
%that follows a normal linear regression model
%xi |zi ? N(z,_i?, ?2)) (1)
%where zi = (zi1, zi2, . . . , ziq)
%records values of predictors of x for observation i, ? is the q × 1vector of unknown regression coefficients, and ?2 is the unknown scalar variance. (Note that when
%a constant is included in the model?the default?zi1 = 1, i = 1, . . . , n.) x contains missing values that are to be filled in. Consider the partition of x = (x0o
%, x'm) into n_0 × 1 and n_1 × 1 vectors containing the complete and the incomplete observations. Consider a similar partition of Z = (Zo, Zm) into n0 × q and n1 × q submatrices.
%mi impute pmm follows the steps below to fill in xm (for simplicity, we omit the conditioning on the observed data in what follows):
%1. Fit a regression model (1) to the observed data (xo, Zo) to obtain estimates ?b and ? 2
%of the model parameters.
%2. Simulate new parameters ? and ?2
%from their joint posterior distribution under the conventional noninformative improper prior Pr(?, ?2 ) ? 1/?2
%. This is done in two steps:
%?^*2 ? ?2
%(n0 ? q)/?2
%n0?q
%?|?2 ? N(n?b, ?2(Z0oZo)?1
%246 mi impute pmm ? Impute using predictive mean matching
%3. Generate the imputed values, x
%1
%m, as follows. Let xbi be the linear prediction of x based
%on predictors Z for observation i. Then for any missing observation i of x, xi = xjmin ,
%where jmin is randomly drawn from the set of indices {i1, i2, . . . , ik} corresponding to
%the first k minimums determined based on the absolute differences between the linear
%prediction for incomplete observation i and linear predictions for all complete observations,
%|xbi ? xbj |, j ? obs. For example, if k = 1 (the default), jmin is determined based on
%|xbi ? xbjmin | = minj?obs|xbi ? xbj |.
%4. Repeat steps 2 and 3 to obtain M sets of imputed values, x
%1
%m, x
%2
%m, . . . , xM
%m 
%Source    Yulia Marchenko (StataCorp) Multiple-imputation analysis using Stata's mi command September 10, 2009  2009 UK Stata Users Group Meeting
% \endinput
% \begin{comment}
% \subsection{Estimation of log linear model and the Poisson pseudo maximum likelihood}
% Further, \textcite{silva} strongly recommended to use the PPML estimator instead of the OLS for estimation problems as  presented in equation 2.1. They showed that  estimating a model equation in a log linear fashion  is only consistent, under very strong functional assumptions about the error terms.  Further, the authors highlighted their argument with Monte Carlo simulations. The simulation results indicated that under heteroscedasticity OLS estimates were biased. The PPML estimator showed unbiased estimates. In the following I  outline the authors argument in a regression model similar to 2.1 . \par
% The stochastic formulation of the log linear model holds for each observation only up to a stochastic error $x_{i}-E(x_{i}|z_{i} \theta)=\epsilon_{i}$. The  model is as follows $x_{i}=\exp(z_{i}  \theta)+\epsilon_{i}$, with  $x_{i} \geq 0$ and $E[\epsilon_{i}|z_{i}]=0.$ \par
% The estimation of the model in this form may be inconsistent for two reasons.
% First, if $x_{i}$ takes values equal to zero. Second the log linear model the error will in general will depend on the covariates. In the following I outline this. Reformulating the model equivalently so that $x_{i}=exp(z_{i} \theta) \eta_i$, where $\eta_{i}$ is defined as $v_{i} * exp(z_{i} \theta)$. Log linearizing this equation  $\log(x_{i})=z_{i} \theta+\eta_{i}$, where  $\eta_i=\exp(z_{i} \theta)*\epsilon_{i}$. This equation is only consistently estimated with OLS, if $\epsilon_{i}=\exp( z_{i}\theta) v_{i}$, where $v_{i}$ is a random variable orthogonal to $x_{i}$. Hence the estimation of the model with OLS requires specific assumptions about the error terms to identify the parameter of interest $\theta$ .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
% \par
% Further, the authors note that if the statistical independence of $x_{i} $of $\eta_i$ shall hold, it implies that the conditional variance of $x_{i}$ is proportional to $exp(2x_{i} \theta)$. Moreover as the log linear model assumes that $x_{i}$ is a positive integer,  the conditional mean and the probability that   $x_{i}$  is zero have to approach zero.  Therefore the variance has to vanish as  $x_{i}$  approaches zero. For large values of $x_{i}$ however large deviance positive and negative are possible. Therefore the consistency of the OLS estimator requires assumption about the second moment of the distribution to be consistent. For the outlined reasons the authors therefore recommend to estimate 
% \par
% In the following I describe the necessary transformations to estimate \ref{eq:1} in the multiplicative form with the PPML estimator.  \par First, I transform the estimation equation into the multiplicative by a simple e-transformation $E[x^k_{i,j} | \tilde{z}^k_{i} , \theta]=exp(\delta_{i,j}+\delta^k_j+\theta \ln \tilde{z}^k_i +\epsilon^k_{i,j})$ . Moreover, following the assumption on the form of heteroscedasticity in \textcite{silva}  I assume that that the conditional mean is proportional to the variance. This assumption implies that  all observations are equally weighted.  Formally, \begin{align} \label{eq:4} E[x^k_{i,j} | \tilde{z}^k_{i} ] \propto V[x^k_{i,j} | \tilde{z}^k_{i} ] \end{align}. \par 
%  The estimator  solves the following first order condition  \begin{align} \label{eq:5} \sum_{i=1}^n  (x^k_{i,j}-exp(\tilde{z}^k_{i} \beta)) x^k_{i,j} = 0 \end{align}  As I outline in the next paragraph the estimator solving the first order condition in \cref{eq:5} is similar  to the Poisson pseudo maximum likelihood function. Further, \textcite{silva} note that the estimation such as \cref{eq:5} is numerically equivalent to the PPML.   \par
% To show that the estimator from \ref{eq:5} is closely related to the PPML, I first briefly review the maximum likelihood estimation (MLE) and subsequently Poisson (pseudo) maximum likelihood estimation. The paragraph closely follows the textbook exposition in \textcite[pp. 117--118]{cameron2009}. \par
% A maximum likelihood estimator (MLE)  maximizes the log likelihood function. The likelihood function is defined as the joint density of observations from a sample  $(y_i, x_i  \quad i=1, \dots, n)$ .  The variable $y_i$ denotes a dependent variable and $x_i$ denotes the independent variables.  Under the assumption of independence, the joint density is the product of the densities of each observation  $\prod_i f(y_i|x_i, \theta) $,and $\theta$ is the parameter of interest. A log transformation of the joint densities transforms the maximization problem above into the sum of the log density \[\sum_{i=1}^n \log[f(y_i|x_i, \theta)] \] 
% The conditional density function of the Poisson distribution is obtained from the probability mass function$ f(y | \mu )=e^\mu \mu^y / y! \quad y=0,1,2 , \dots $. Further the Poisson distribution has the property of equi-dispersion, which is $E[y_i] = \mu  \quad  \text{and} \quad V[y_i] = \mu  \leftrightarrow E[y_i] = V[y_i]$ 
% In the Poisson regression model  the parameter $\mu$ is usually specified as  $\mu=exp(x'\theta)$. Therefore for a single observation the density is \[ f(y_i|x_i,\theta)=\frac{e^{-exp(x_i ' \theta)} exp(x_i'\theta)^y_i}{y_i !}\] \par 
%  The Poisson maximum likelihood estimator $\theta$ maximizes the sum of the log densities of the Poisson distribution  \[ log[L(\theta)]= \frac{1}{N}\sum_{i=1}^n  \{ - exp (x_i'\theta) + y_i x_i'  \theta - \ln y_i! \} .\] The ML estimator $\theta$ therefore solves the following first order conditions \begin{align} \label{eq:qmle}\sum_{i=1}^n ( y_i- exp(x_i \theta)) x_i |_{\hat{\theta}}=0\end{align} The equation has no explicit solution and is therefore solved with numerical methods. As one can see the  first order condition \cref{eq:qmle} and the Poisson maximum likelihood first order condition in \cref{eq:5} are equivalent.  \par 
% Even if the outline before I specifed the dependent variable as a count variable, this is not a necessary conditon for estimating this model. It is  
%   To obtain a correct maximum likelihood estimator the log likelihood function has to be correctly specified. The Poisson maximum likelihood is correct specified if both the conditional mean is correctly specified $E[y_i|x_i]=exp(x' \theta)$ and the equi-dispersion assumption $E[y_i]=V[y_i]$  holds. \textcite{silva} noted, it is unlikely that the latter assumption holds. \par The pseudo maximum likelihood estimation is defined as the maximum likelihood estimation maximizing a misspecified log likelihood \parencite[p.465]{hayashi2011}, which may be a first order or a second order taylor approximation of the true likelihood.  Even if the maximum likelihood is misspecified, it can be shown that the pseudo maximum likelihood estimator is consistent under the condition that ${y_i \ \ \& \ \ x_i}$ are ergodic stationary. Further,
% the pseudo maximum likelihood is consistent without the assumption of identical and independent distributed error terms \parencite[p.465]{hayashi2011}.
% %%The international price data of the GGDC database is based on a large sample of gross relative output prices, which are the quotient of purchasing power parities divided by the exchange rate and relative to the USA GDP price level. \textcite{Timmer2012}  obtained the the international sector output prices by estimating reference prices and purchasing power parities at the industry level using a large sample of nominal values of final expenditure and prices collected from various sources.  The estimation procedure is an extension of the system simultaneous equations in \textcite{Feenstra}. In my analysis, I use the inverse of this relative gross output price as an empirical measure of productivity. 
% \end{comment}
\endinput